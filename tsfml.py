# -*- coding: utf-8 -*-
"""TSFML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sC9IKZJPa8gl6qoByiSgX_VKPZc_DlR4
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense

d1 = pd.read_csv("/content/April (Serialized + Stationarized + Spanned) (1).csv")
d2 = pd.read_csv("/content/May (Serialized + Stationarized + Spanned) (1).csv")
d3 = pd.read_csv("/content/June (Serialized + Stationarized + Spanned) (1).csv")
d4 = pd.read_csv("/content/July (Serialized + Stationarized + Spanned) (1).csv")

frames = [d1, d2, d3,d4]
df = pd.concat(frames).drop('zeitstampel', 1).to_numpy().flatten()

def preprocess(data):
  value = list()
  lag1 = list()
  lag2 = list()
  lag3 = list()
  # lag4 = list()
  # lag5 = list()
  # lag6 = list()
  # lag7 = list()
  # lag8 = list()
  # lag9 = list()
  # lag10 = list()
  # lag11 = list()
  # lag12 = list()

  nzInt = list()
  zCum = list()
  i = 3
  while i < len(data):
    value.append(data[i])
    lag1.append(data[i-1])
    lag2.append(data[i-2])
    lag3.append(data[i-3])
    # lag4.append(data[i-4])
    # lag5.append(data[i-5])
    # lag6.append(data[i-6])
    # lag7.append(data[i-7])
    # lag8.append(data[i-8])
    # lag9.append(data[i-9])
    # lag10.append(data[i-10])
    # lag11.append(data[i-11])
    # lag12.append(data[i-12])


    nzInt.append(getInterval(data, i-1))
    zCum.append(getZeros(data, i-1))
    i+=1
    

  d = {"lag1": lag1,
       "lag2": lag2,
       "lag3": lag3,
      #  "lag4": lag4,
      #  "lag5": lag5,
      #  "lag6": lag6,
      #  "lag7": lag7,
      #  "lag8": lag8,
      #  "lag9": lag9,
      #  "lag10": lag10,
      #  "lag11": lag11,
      #  "lag12": lag12,
       "nzInt": nzInt,
       "zCum": zCum,
       "value": value
       }

  df = pd.DataFrame(data=d)
  return df

def getInterval(data, index):
  count = 1

  while (index >=0):
    if (data[index]!=0):
      break
    index-=1

  if (index == 0):
    return -1

  index-=1

  while (index>=0):
    if (data[index]!=0):
      break
    else:
      index-=1
      count+=1
  return count

def getZeros(data, index):
  count = 0 
  while (index >= 0):
    if (data[index] == 0):
      count+=1
    else:
      break
    index-=1
  return count

df = preprocess(df)

X = df.iloc[:,:-1]
y = df.iloc[: , -1]

X.iloc[:,:3] = X.iloc[:,:3]/np.amax(df.iloc[:,0], axis=0)
X.iloc[:,3:5] = X.iloc[:,3:5]/10
y = y/np.amax(df.iloc[:,0], axis=0)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle = False)

model = Sequential()
model.add(Dense(10,'sigmoid', input_dim=5))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])

"""good choices:
N = 7, tanh, batchsize=24&&N=5


"""

model.fit(X_train, y_train,epochs=200)

yhat = model.predict(X_test, verbose=0)

aSum = list()
pSum = list()
yhat = np.array(yhat.flatten())

for i in range(len(yhat)):
  if(yhat[i]<0):
    yhat[i] = 0

# for i in range(len(yhat)):
#   if yhat[i] < 20:
#     yhat[i] = 0
i = 12
while i <= (len(yhat)-24):
  a = np.sum(y_test[i:i+24])*265
  p = np.sum(yhat[i:i+24])*265
  aSum.append(a)
  pSum.append(p)
  i+=24

for i in range(len(aSum)):
  print('Actual: ', aSum[i], 'Predicted: ', pSum[i], 'Loss: ', (pSum[i]-aSum[i]))

sum = 0
for i in range(len(aSum)):
  sum+=abs(pSum[i]-aSum[i])
print("Total Error: ", sum)
print("Durchschnittlicher taeglicher Fehler: ", sum/30)

x = list(range(0,30))
i = 0
fig, ax = plt.subplots()
# ax.scatter(x, y_test[i:i+30]*265)
# ax.scatter(x, yhat[i:i+30]*265)
ax.scatter(x, aSum)
ax.scatter(x, pSum)
plt.show()

